{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca525ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import transformers\n",
    "import datasets\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c51a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"The mother woke the child.\"\n",
    "text2 = \"The child woke his mother.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39ca350a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence (AI) is transforming the world at an unprecedented pace. From self-driving cars to personalized healthcare recommendations, AI-powered technologies are reshaping industries and redefining how we live and work. In education, AI tools help identify student learning patterns and customize content accordingly. In finance, algorithms detect fraudulent transactions in real-time. AI is also being used in agriculture to monitor crop health and optimize irrigation. However, with great power comes great responsibility. Concerns around data privacy, bias in decision-making, and job displacement are growing. To ensure ethical use, it’s essential for governments, organizations, and developers to collaborate on transparent policies. As AI continues to evolve, a balanced approach will help society maximize benefits while minimizing risks.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"exp_text.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "  \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1612d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 0]\n",
      "minimizing<unk>\n"
     ]
    }
   ],
   "source": [
    "# oluşturulan tokenizer'ı kullanarak metinleri token'lara ayırma\n",
    "from tokenizer import Tokenizer\n",
    "create_tokenizer = Tokenizer(\"exp_text.json\")\n",
    "print(create_tokenizer.encode(\"minimizing.\"))\n",
    "print(create_tokenizer.decode([100, 0]))\n",
    "#vocab json dosyasında minimizing kelimesi var ancak bunu text'te minimiz ing olarak yazdığımıda vocab'ta karşığı olmamasına rağmen kombinasyon ile oluşturabiliyor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c625cf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 1, 4, 1, 5, 1, 6, 1, 7, 1, 8, 1, 9, 1, 10, 1, 11, 1, 12, 1, 13, 1, 14, 1, 15, 1, 16, 1, 17, 1, 18, 1, 19, 1, 20, 1, 21, 1, 22, 1, 23, 1, 24, 1, 25, 1, 26, 1, 27, 1, 28, 1, 29, 1, 25, 1, 30, 1, 31, 1, 32, 1, 33, 1, 34, 1, 35, 1, 36, 1, 37, 1, 38, 1, 39, 1, 25, 1, 40, 1, 41, 1, 42, 1, 31, 1, 43, 1, 44, 1, 45, 1, 46, 1, 47, 1, 48, 1, 49, 1, 33, 1, 5, 1, 50, 1, 51, 1, 52, 1, 48, 1, 53, 1, 16, 1, 54, 1, 55, 1, 56, 1, 25, 1, 57, 1, 58, 1, 59, 1, 60, 1, 61, 1, 62, 1, 63, 1, 61, 1, 64, 1, 65, 1, 66, 1, 67, 1, 68, 1, 69, 1, 48, 1, 70, 1, 25, 1, 71, 1, 72, 1, 22, 1, 73, 1, 74, 1, 75, 1, 76, 1, 77, 1, 78, 1, 79, 1, 80, 1, 81, 1, 82, 1, 25, 1, 83, 1, 16, 1, 84, 1, 85, 1, 86, 1, 87, 1, 88, 1, 33, 1, 89, 1, 16, 1, 90, 1, 91, 1, 92, 1, 93, 1, 94, 1, 35, 1, 96, 1, 97, 1, 98, 1, 99, 1, 100, 1, 101]\n"
     ]
    }
   ],
   "source": [
    "create_tokens = create_tokenizer.encode(text)\n",
    "print(create_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d31da",
   "metadata": {},
   "source": [
    "#### basic bir model eğitmek istersek: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3721c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm # google'ın geliştirdiği bir tokenizer kütüphanesi\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input='exp_text.txt', \n",
    "    model_prefix='spm_model', # model dosyasının ismi\n",
    "    vocab_size=200, # kaç token olacağını belirliyor\n",
    "    model_type='bpe' # bpe, unigram, char, word (bpe: byte pair encoding)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85150f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 169, 81, 77, 12, 33, 167, 28, 64, 157, 13, 126, 99, 192, 86, 97, 155, 176, 10, 9, 109, 91, 114, 4, 167, 4, 165, 129, 118, 5, 73, 25, 38, 23, 130, 181, 162, 100, 39, 54, 149, 189, 173, 80, 10, 15, 134, 35, 23, 76, 6, 12, 20, 38, 98, 172, 72, 34, 143, 176, 150, 95, 180, 44, 189, 175, 30, 151, 173, 9, 60, 111, 29, 168, 108, 18, 90, 34, 122, 103, 10, 33, 173, 83, 21, 18, 26, 34, 145, 3, 10, 32, 30, 24, 163, 162, 64, 71, 26, 91, 190, 181, 84, 162, 38, 125, 45, 180, 44, 35, 29, 170, 94, 43, 146, 81, 188, 54, 69, 173, 25, 162, 115, 133, 10, 23, 137, 17, 65, 26, 15, 83, 39, 55, 93, 25, 4, 107, 11, 173, 10, 116, 181, 84, 52, 3, 16, 13, 180, 85, 158, 78, 117, 31, 47, 148, 52, 169, 138, 70, 25, 97, 46, 36, 170, 33, 34, 12, 189, 82, 163, 181, 44, 86, 85, 123, 41, 163, 10, 87, 38, 33, 4, 159, 144, 69, 5, 35, 53, 6, 19, 11, 15, 169, 50, 98, 26, 162, 50, 82, 55, 43, 169, 21, 174, 45, 181, 128, 30, 48, 17, 180, 24, 78, 96, 23, 30, 17, 15, 39, 18, 96, 34, 51, 6, 67, 139, 19, 188, 181, 127, 6, 142, 65, 4, 66, 178, 14, 31, 135, 23, 80, 132, 180, 41, 161, 33, 31, 147, 67, 6, 189, 176, 102, 10, 180, 26, 162, 112, 186, 31, 164, 51, 63, 13, 88, 90, 42, 169, 30, 10, 181, 162, 101, 162, 7, 124, 5, 162, 47, 110, 141, 87, 163, 180, 162, 19, 199, 170, 162, 153, 7, 8, 12, 52, 11, 42, 120, 152, 88, 170, 180, 162, 11, 156, 20, 95, 180, 26, 31, 48, 28, 50, 76, 35, 15, 29, 63, 140, 136, 162, 6, 57, 51, 72, 119, 23, 29, 77, 18, 181, 22, 170, 44, 93, 3, 178, 18, 35, 162, 48, 29, 71, 180, 4, 41, 12, 16, 73, 4, 121, 66, 131, 24, 49, 171, 94, 54, 168, 37, 154, 53, 106, 62, 55, 41, 7, 61, 19, 170, 24, 160, 163, 53, 3, 62, 20, 10, 162, 21, 170, 113, 181]\n",
      "['▁A', 'r', 'tif', 'ici', 'al', '▁in', 't', 'el', 'li', 'gen', 'ce', '▁(', 'AI', ')', '▁is', '▁trans', 'for', 'm', 'ing', '▁t', 'he', '▁wor', 'ld', '▁a', 't', '▁a', 'n', '▁u', 'np', 're', 'ced', 'ent', 'ed', '▁p', 'ace', '.', '▁', 'Fr', 'om', '▁s', 'elf', '-', 'd', 'riv', 'ing', '▁c', 'ars', '▁to', '▁p', 'ers', 'on', 'al', 'iz', 'ed', '▁health', 'c', 'are', '▁re', 'com', 'm', 'end', 'ations', ',', '▁AI', '-', 'p', 'ow', 'ere', 'd', '▁t', 'ec', 'hn', 'ol', 'o', 'gi', 'es', '▁are', '▁re', 'sh', 'ap', 'ing', '▁in', 'd', 'ust', 'ri', 'es', '▁and', '▁re', 'def', 'in', 'ing', '▁h', 'ow', '▁w', 'e', '▁', 'li', 've', '▁and', '▁wor', 'k', '.', '▁In', '▁', 'ed', 'uc', 'ation', ',', '▁AI', '▁to', 'ol', 's', '▁help', '▁i', 'den', 'tif', 'y', '▁s', 'tu', 'd', 'ent', '▁', 'le', 'arn', 'ing', '▁p', 'att', 'er', 'ns', '▁and', '▁c', 'ust', 'om', 'ize', '▁cont', 'ent', '▁a', 'cc', 'or', 'd', 'ing', 'ly', '.', '▁In', '▁f', 'in', 'an', 'ce', ',', '▁al', 'gor', 'ith', 'ms', '▁d', 'et', 'ect', '▁f', 'r', 'aud', 'ul', 'ent', '▁trans', 'ac', 'tion', 's', '▁in', '▁re', 'al', '-', 'tim', 'e', '.', '▁AI', '▁is', '▁al', 'so', '▁b', 'e', 'ing', '▁us', 'ed', '▁in', '▁a', 'gri', 'cul', 'tu', 're', '▁to', '▁m', 'on', 'it', 'or', '▁c', 'r', 'op', '▁health', '▁and', '▁', 'op', 'tim', 'ize', '▁i', 'r', 'ri', 'g', 'ation', '.', '▁H', 'ow', 'ev', 'er', ',', '▁w', 'ith', '▁great', '▁p', 'ow', 'er', '▁c', 'om', 'es', '▁great', '▁re', 'sp', 'on', 'si', 'bil', 'it', 'y', '.', '▁C', 'on', 'cer', 'ns', '▁a', 'ro', 'u', 'nd', '▁d', 'ata', '▁p', 'riv', 'acy', ',', '▁b', 'ias', '▁in', '▁d', 'eci', 'si', 'on', '-', 'm', 'ak', 'ing', ',', '▁and', '▁', 'jo', 'b', '▁d', 'i', 'sp', 'la', 'ce', 'ment', '▁are', '▁g', 'r', 'ow', 'ing', '.', '▁', 'To', '▁', 'en', 'su', 're', '▁', 'et', 'hi', 'cal', '▁us', 'e', ',', '▁', 'it', '’', 's', '▁', 'ess', 'en', 'ti', 'al', '▁f', 'or', '▁g', 'ov', 'ern', 'ment', 's', ',', '▁', 'or', 'gan', 'iz', 'ations', ',', '▁and', '▁d', 'ev', 'el', 'op', 'ers', '▁to', '▁c', 'ol', 'la', 'bor', 'ate', '▁', 'on', '▁tran', 'sp', 'are', 'nt', '▁p', 'ol', 'ici', 'es', '.', '▁A', 's', '▁AI', '▁cont', 'in', 'u', 'es', '▁to', '▁', 'ev', 'ol', 've', ',', '▁a', '▁b', 'al', 'an', 'ced', '▁a', 'pp', 'ro', 'ach', '▁w', 'il', 'l', '▁help', '▁s', 'o', 'ci', 'ety', '▁m', 'ax', 'im', 'ize', '▁b', 'en', 'ef', 'it', 's', '▁w', 'hil', 'e', '▁m', 'in', 'im', 'iz', 'ing', '▁', 'ri', 's', 'ks', '.']\n"
     ]
    }
   ],
   "source": [
    "spm_tokenizer = spm.SentencePieceProcessor(model_file='spm_model.model')\n",
    "print(spm_tokenizer.encode(text))\n",
    "spm_tokens = spm_tokenizer.encode(text, out_type=str)\n",
    "print(spm_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "578e3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91ad1d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = Tokenizer(BPE()) # BPE modelini kullanarak bir tokenizer oluştur\n",
    "hf_tokenizer.pre_tokenizer = Whitespace() # boşluklara göre token'lara ayırma işlemi yapacak\n",
    "trainer = BpeTrainer(vocab_size=8, special_tokens=[\"<unk>\"]) # vocab boyutu ve özel token'lar\n",
    "hf_tokenizer.train([\"exp_text.txt\"], trainer) # eğitimi başlat\n",
    "hf_tokenizer.save(\"hf_tokenizer.json\") # eğitilen tokenizer'ı kaydet,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7afc6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "[6, 28, 30, 20, 17, 20, 14, 20, 12, 23, 20, 25, 30, 16, 23, 23, 20, 18, 16, 25, 14, 16, 1, 6, 10, 2, 20, 29, 30, 28, 12, 25, 29, 17, 26, 28, 24, 20, 25, 18, 30, 19, 16, 33, 26, 28, 23, 15, 12, 30, 12, 25, 31, 25, 27, 28, 16, 14, 16, 15, 16, 25, 30, 16, 15, 27, 12, 14, 16, 5, 8, 28, 26, 24, 29, 16, 23, 17, 4, 15, 28, 20, 32, 20, 25, 18, 14, 12, 28, 29, 30, 26, 27, 16, 28, 29, 26, 25, 12, 23, 20, 36, 16, 15, 19, 16, 12, 23, 30, 19, 14, 12, 28, 16, 28, 16, 14, 26, 24, 24, 16, 25, 15, 12, 30, 20, 26, 25, 29, 3, 6, 10, 4, 27, 26, 33, 16, 28, 16, 15, 30, 16, 14, 19, 25, 26, 23, 26, 18, 20, 16, 29, 12, 28, 16, 28, 16, 29, 19, 12, 27, 20, 25, 18, 20, 25, 15, 31, 29, 30, 28, 20, 16, 29, 12, 25, 15, 28, 16, 15, 16, 17, 20, 25, 20, 25, 18, 19, 26, 33, 33, 16, 23, 20, 32, 16, 12, 25, 15, 33, 26, 28, 22, 5, 10, 25, 16, 15, 31, 14, 12, 30, 20, 26, 25, 3, 6, 10, 30, 26, 26, 23, 29, 19, 16, 23, 27, 20, 15, 16, 25, 30, 20, 17, 35, 29, 30, 31, 15, 16, 25, 30, 23, 16, 12, 28, 25, 20, 25, 18, 27, 12, 30, 30, 16, 28, 25, 29, 12, 25, 15, 14, 31, 29, 30, 26, 24, 20, 36, 16, 14, 26, 25, 30, 16, 25, 30, 12, 14, 14, 26, 28, 15, 20, 25, 18, 23, 35, 5, 10, 25, 17, 20, 25, 12, 25, 14, 16, 3, 12, 23, 18, 26, 28, 20, 30, 19, 24, 29, 15, 16, 30, 16, 14, 30, 17, 28, 12, 31, 15, 31, 23, 16, 25, 30, 30, 28, 12, 25, 29, 12, 14, 30, 20, 26, 25, 29, 20, 25, 28, 16, 12, 23, 4, 30, 20, 24, 16, 5, 6, 10, 20, 29, 12, 23, 29, 26, 13, 16, 20, 25, 18, 31, 29, 16, 15, 20, 25, 12, 18, 28, 20, 14, 31, 23, 30, 31, 28, 16, 30, 26, 24, 26, 25, 20, 30, 26, 28, 14, 28, 26, 27, 19, 16, 12, 23, 30, 19, 12, 25, 15, 26, 27, 30, 20, 24, 20, 36, 16, 20, 28, 28, 20, 18, 12, 30, 20, 26, 25, 5, 9, 26, 33, 16, 32, 16, 28, 3, 33, 20, 30, 19, 18, 28, 16, 12, 30, 27, 26, 33, 16, 28, 14, 26, 24, 16, 29, 18, 28, 16, 12, 30, 28, 16, 29, 27, 26, 25, 29, 20, 13, 20, 23, 20, 30, 35, 5, 7, 26, 25, 14, 16, 28, 25, 29, 12, 28, 26, 31, 25, 15, 15, 12, 30, 12, 27, 28, 20, 32, 12, 14, 35, 3, 13, 20, 12, 29, 20, 25, 15, 16, 14, 20, 29, 20, 26, 25, 4, 24, 12, 22, 20, 25, 18, 3, 12, 25, 15, 21, 26, 13, 15, 20, 29, 27, 23, 12, 14, 16, 24, 16, 25, 30, 12, 28, 16, 18, 28, 26, 33, 20, 25, 18, 5, 11, 26, 16, 25, 29, 31, 28, 16, 16, 30, 19, 20, 14, 12, 23, 31, 29, 16, 3, 20, 30, 37, 29, 16, 29, 29, 16, 25, 30, 20, 12, 23, 17, 26, 28, 18, 26, 32, 16, 28, 25, 24, 16, 25, 30, 29, 3, 26, 28, 18, 12, 25, 20, 36, 12, 30, 20, 26, 25, 29, 3, 12, 25, 15, 15, 16, 32, 16, 23, 26, 27, 16, 28, 29, 30, 26, 14, 26, 23, 23, 12, 13, 26, 28, 12, 30, 16, 26, 25, 30, 28, 12, 25, 29, 27, 12, 28, 16, 25, 30, 27, 26, 23, 20, 14, 20, 16, 29, 5, 6, 29, 6, 10, 14, 26, 25, 30, 20, 25, 31, 16, 29, 30, 26, 16, 32, 26, 23, 32, 16, 3, 12, 13, 12, 23, 12, 25, 14, 16, 15, 12, 27, 27, 28, 26, 12, 14, 19, 33, 20, 23, 23, 19, 16, 23, 27, 29, 26, 14, 20, 16, 30, 35, 24, 12, 34, 20, 24, 20, 36, 16, 13, 16, 25, 16, 17, 20, 30, 29, 33, 19, 20, 23, 16, 24, 20, 25, 20, 24, 20, 36, 20, 25, 18, 28, 20, 29, 22, 29, 5]\n"
     ]
    }
   ],
   "source": [
    "print(hf_tokenizer.get_vocab_size())\n",
    "print(hf_tokenizer.encode(text).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f28769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 28, 30, 20, 17, 20, 14, 20, 12, 23, 20, 25, 30, 16, 23, 23, 20, 18, 16, 25, 14, 16, 1, 6, 10, 2, 20, 29, 30, 28, 12, 25, 29, 17, 26, 28, 24, 20, 25, 18, 30, 19, 16, 33, 26, 28, 23, 15, 12, 30, 12, 25, 31, 25, 27, 28, 16, 14, 16, 15, 16, 25, 30, 16, 15, 27, 12, 14, 16, 5, 8, 28, 26, 24, 29, 16, 23, 17, 4, 15, 28, 20, 32, 20, 25, 18, 14, 12, 28, 29, 30, 26, 27, 16, 28, 29, 26, 25, 12, 23, 20, 36, 16, 15, 19, 16, 12, 23, 30, 19, 14, 12, 28, 16, 28, 16, 14, 26, 24, 24, 16, 25, 15, 12, 30, 20, 26, 25, 29, 3, 6, 10, 4, 27, 26, 33, 16, 28, 16, 15, 30, 16, 14, 19, 25, 26, 23, 26, 18, 20, 16, 29, 12, 28, 16, 28, 16, 29, 19, 12, 27, 20, 25, 18, 20, 25, 15, 31, 29, 30, 28, 20, 16, 29, 12, 25, 15, 28, 16, 15, 16, 17, 20, 25, 20, 25, 18, 19, 26, 33, 33, 16, 23, 20, 32, 16, 12, 25, 15, 33, 26, 28, 22, 5, 10, 25, 16, 15, 31, 14, 12, 30, 20, 26, 25, 3, 6, 10, 30, 26, 26, 23, 29, 19, 16, 23, 27, 20, 15, 16, 25, 30, 20, 17, 35, 29, 30, 31, 15, 16, 25, 30, 23, 16, 12, 28, 25, 20, 25, 18, 27, 12, 30, 30, 16, 28, 25, 29, 12, 25, 15, 14, 31, 29, 30, 26, 24, 20, 36, 16, 14, 26, 25, 30, 16, 25, 30, 12, 14, 14, 26, 28, 15, 20, 25, 18, 23, 35, 5, 10, 25, 17, 20, 25, 12, 25, 14, 16, 3, 12, 23, 18, 26, 28, 20, 30, 19, 24, 29, 15, 16, 30, 16, 14, 30, 17, 28, 12, 31, 15, 31, 23, 16, 25, 30, 30, 28, 12, 25, 29, 12, 14, 30, 20, 26, 25, 29, 20, 25, 28, 16, 12, 23, 4, 30, 20, 24, 16, 5, 6, 10, 20, 29, 12, 23, 29, 26, 13, 16, 20, 25, 18, 31, 29, 16, 15, 20, 25, 12, 18, 28, 20, 14, 31, 23, 30, 31, 28, 16, 30, 26, 24, 26, 25, 20, 30, 26, 28, 14, 28, 26, 27, 19, 16, 12, 23, 30, 19, 12, 25, 15, 26, 27, 30, 20, 24, 20, 36, 16, 20, 28, 28, 20, 18, 12, 30, 20, 26, 25, 5, 9, 26, 33, 16, 32, 16, 28, 3, 33, 20, 30, 19, 18, 28, 16, 12, 30, 27, 26, 33, 16, 28, 14, 26, 24, 16, 29, 18, 28, 16, 12, 30, 28, 16, 29, 27, 26, 25, 29, 20, 13, 20, 23, 20, 30, 35, 5, 7, 26, 25, 14, 16, 28, 25, 29, 12, 28, 26, 31, 25, 15, 15, 12, 30, 12, 27, 28, 20, 32, 12, 14, 35, 3, 13, 20, 12, 29, 20, 25, 15, 16, 14, 20, 29, 20, 26, 25, 4, 24, 12, 22, 20, 25, 18, 3, 12, 25, 15, 21, 26, 13, 15, 20, 29, 27, 23, 12, 14, 16, 24, 16, 25, 30, 12, 28, 16, 18, 28, 26, 33, 20, 25, 18, 5, 11, 26, 16, 25, 29, 31, 28, 16, 16, 30, 19, 20, 14, 12, 23, 31, 29, 16, 3, 20, 30, 37, 29, 16, 29, 29, 16, 25, 30, 20, 12, 23, 17, 26, 28, 18, 26, 32, 16, 28, 25, 24, 16, 25, 30, 29, 3, 26, 28, 18, 12, 25, 20, 36, 12, 30, 20, 26, 25, 29, 3, 12, 25, 15, 15, 16, 32, 16, 23, 26, 27, 16, 28, 29, 30, 26, 14, 26, 23, 23, 12, 13, 26, 28, 12, 30, 16, 26, 25, 30, 28, 12, 25, 29, 27, 12, 28, 16, 25, 30, 27, 26, 23, 20, 14, 20, 16, 29, 5, 6, 29, 6, 10, 14, 26, 25, 30, 20, 25, 31, 16, 29, 30, 26, 16, 32, 26, 23, 32, 16, 3, 12, 13, 12, 23, 12, 25, 14, 16, 15, 12, 27, 27, 28, 26, 12, 14, 19, 33, 20, 23, 23, 19, 16, 23, 27, 29, 26, 14, 20, 16, 30, 35, 24, 12, 34, 20, 24, 20, 36, 16, 13, 16, 25, 16, 17, 20, 30, 29, 33, 19, 20, 23, 16, 24, 20, 25, 20, 24, 20, 36, 20, 25, 18, 28, 20, 29, 22, 29, 5]\n",
      "[11, 19, 16, 24, 26, 30, 19, 16, 28, 33, 26, 22, 16, 30, 19, 16, 14, 19, 20, 23, 15, 5]\n"
     ]
    }
   ],
   "source": [
    "# oluşturulan tokenizer'ı kullanarak metinleri token'lara ayırma\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "hf_pretrained_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"hf_tokenizer.json\")\n",
    "print(hf_pretrained_tokenizer.encode(text))\n",
    "print(hf_pretrained_tokenizer.encode(text1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
